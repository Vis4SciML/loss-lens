# Server Pipeline Instructions
This is a quick instruction to run examples in the whole pipeline.

## 2-Layer MLP model with MNIST/MNIST-C

### Training model Instructions
This example will train a 2-layers MLP model with [MNIST](https://pytorch.org/vision/stable/generated/torchvision.datasets.MNIST.html#torchvision.datasets.MNIST) and [MNIST-C](https://github.com/google-research/mnist-c) datasets. The models are defined as follows.

```python
class MLPSmall(torch.nn.Module):
    def __init__(self, x_dim, y_dim):
        super().__init__()
        self.linear_1 = torch.nn.Linear(x_dim, 32)
        self.linear_2 = torch.nn.Linear(32, y_dim)

    def forward(self, x):
        h = F.relu(self.linear_1(x))
        return F.softmax(self.linear_2(h), dim=1)
```

The MNIST-C dataset has 16 different kinds of corruptions and if we add the original MNIST dataset and add one more example which contains all the MNIST and MNIST-C datasets, we will have 18 input training datasets in total, which will generate 18 trained models in this example. The training process can be excute as follows under the `train` folder in the upper level directory.

```shell
python3 train_mlp_mnist.py --dataset=<training input dataset>
```

The training input dataset options are `original`, `brightness`, `canny_edges`, `dotted_line`, `fog`, `glass_blur`, `identity`, `impulse_noise`, `motion_blur`, `rotate`, `scale`, `shear`, `shot_noise`, `spatter`, `stripe`, `translate`, `zigzag`, `all`. The default setting is `original`.

We do provide a script file which contains all the commands to train all the models in MNIST and MNIST-C. You could just run the following command to obtain all the trained MNIST and MNIST-C models.

```shell
sh run_train_mlp_mnist.sh
```

One `model_initial.pt` and one `model_final.pt` will be generated under the corresponding folder which should be under `model/MNIST` folder. For example, two models of `original` training option will be generated under `model/MNIST/original` folder.

### Visualization Instructions
In this tool, the main parts of visualization include model and layer similarity(Model Ensemble View, Model Similarity View and Layer Similarity View), model information(Evaluation View), 2D and 3D loss landscapes(Loss Landscape View) and prediction distribuation(Model Analysis View). For model and layer similarity, this tool provides euclidean distance similarity for models only. We also provide linear and kernel [CKA](https://arxiv.org/abs/1905.00414) similarity for models and CKA similarity for layers as well. For the model information for each model, this system does provide some information related to the model and data, which needs to be visualized. The model information mainly includes `accuracy`, `recall`, `precision`, `f1 score`, `confusion matrix`, and `top-2 Hessian eigenvalues`. For 2D and 3D loss landscapes, we do provide the classic 2D loss landscapes for different models and compare the similarity among them using random projection method. Besides, we provide the merge tree and persistant diagram for 3D loss landscapes. For prediction distribuation, different models in the center of previous loss landscapes will predict the testing data and give different predictions. Model Analysis View presents the differences among all those results. You can find more details in the [Back-End Operations Instructions](operation/README.md#back-end-operations-instructions).

### Main Parts Instructions
To run model and layer similarity, model information, 2D loss landscapes and prediction distribuation for the 2-Layers MLP with MNIST, run the following Python script under the current folder.

```shell
python3 calculate_mlp_mnist.py
```

All the results will be saved in the MongoDB and all the generated model files will be saved under the `model_list` folder.

### TTK Merge Tree Instructions
One visualization in the 3D loss landscapes is the `Merge Tree`, which is generated by the [TTK](https://topology-tool-kit.github.io/). We will generated the binary files under the `ttk/input_binary_for_ttk/` folder while running the `calculate_mlp_mnist.py`. `TTK` will take the binary files to generate a CSV file in the [ParaView](https://www.paraview.org/). Here, we will show how to obtain meaningful information from the output CSV file and store the information which Front-End might need in the MongoDB. The output CSV files are under `ttk/output_csv_from_ttk/` folder. To obtain meaningful information, we need to run `load_from_csv_mergetree.py` by giving a CSV file path.

```shell
python3 load_from_csv_mergetree.py --file=<input CSV file path>
```

A script file which contains several example commands can be run as following.

```shell
sh run_load_from_csv_mergetree.sh
```

All the results will be saved in the MongoDB. `load_from_csv_mergetree_nersc.py` and `run_load_from_csv_mergetree_nersc.sh` are designed for running this script in `NERSC`. `load_from_csv_mergetree_nersc_training.py` and `run_load_from_csv_mergetree_nersc_training.sh` are designed for running the 3D loss landscapes merge tree in `NERSC`.

### TTK Persistant Diagram Instructions
Another visualization in the 3D loss landscapes is the `Persistant Diagram`, which is also generated by the [TTK](https://topology-tool-kit.github.io/). We will generated the binary files under the `ttk/input_binary_for_ttk/` folder while running the `calculate_mlp_mnist.py`. `TTK` will take the binary files to generate a CSV file in the [ParaView](https://www.paraview.org/). Here, we will show how to obtain meaningful information from the output CSV file and store the information which Front-End might need in the MongoDB. The output CSV files are under `ttk/output_csv_from_ttk/` folder. To obtain meaningful information, we need to run `load_from_csv_persistantdiagram.py` by giving a CSV file path.

```shell
python3 load_from_csv_persistantdiagram.py --file=<input CSV file path>
```

A script file which contains several example commands can be run as following.

```shell
sh run_load_from_csv_persistantdiagram.sh
```

All the results will be saved in the MongoDB. `load_from_csv_persistantdiagram_nersc.py` and `run_load_from_csv_persistantdiagram_nersc.sh` are designed for running this script in `NERSC`. `load_from_csv_persistantdiagram_nersc_training.py` and `run_load_from_csv_persistantdiagram_nersc_training.sh` are designed for running the 3D loss landscapes merge tree in `NERSC`.

## ResNet-18 model with CIFAR10/CIFAR10-C

### Training model Instructions
This example will train several ResNet-18 models with [CIFAR10](https://pytorch.org/vision/stable/generated/torchvision.datasets.CIFAR10.html#torchvision.datasets.CIFAR10) and [CIFAR10-C](https://zenodo.org/record/2535967#.Y5oWsezMJPs) datasets. The model structure is obtained from [pytorchcv.model_provider](https://pypi.org/project/pytorchcv/). The models are defined as follows.

```python
model = torchvision.models.resnet18(weights = None)
model.fc = nn.Linear(512, num_classes)
```

We provide a script which contains several commands to train all the ResNet-18 models. You could just run the following command to obtain those trained models with different percents of CIFAR10-C or trained your own models with your settings.

```shell
sh run_train_resnet18_cifar10.sh
```

You could find more details of functions under `operation` folder. The trained model will be generated under the corresponding folder which should be under `model/CIFAR10/RESNET18` folder.

### Visualization Instructions
In this tool, the main parts of visualization include model and layer similarity(Model Ensemble View, Model Similarity View and Layer Similarity View), model information(Evaluation View), 2D and 3D loss landscapes(Loss Landscape View) and prediction distribuation(Model Analysis View). For model and layer similarity, this tool provides euclidean distance similarity for models only. We also provide linear and kernel [CKA](https://arxiv.org/abs/1905.00414) similarity for models and CKA similarity for layers as well. For the model information for each model, this system does provide some information related to the model and data, which needs to be visualized. The model information mainly includes `accuracy`, `recall`, `precision`, `f1 score`, `confusion matrix`, and `top-2 Hessian eigenvalues`. For 2D and 3D loss landscapes, we do provide the classic 2D loss landscapes for different models and compare the similarity among them using random projection method. Besides, we provide the merge tree and persistant diagram for 3D loss landscapes. For prediction distribuation, different models in the center of previous loss landscapes will predict the testing data and give different predictions. Model Analysis View presents the differences among all those results. You can find more details in the [Back-End Operations Instructions](operation/README.md#back-end-operations-instructions).

### Main Parts Instructions
To run model and layer similarity, model information, 2D loss landscapes and prediction distribuation for the 2-Layers MLP with MNIST, run the following Python script under the current folder.

```shell
python3 calculate_torchvision_cifar10.py
```

All the results will be saved in the MongoDB and all the generated model files will be saved under the `model_list` folder.

### TTK Merge Tree Instructions
One visualization in the 3D loss landscapes is the `Merge Tree`, which is generated by the [TTK](https://topology-tool-kit.github.io/). We will generated the binary files under the `ttk/input_binary_for_ttk/` folder while running the `calculate_mlp_mnist.py`. `TTK` will take the binary files to generate a CSV file in the [ParaView](https://www.paraview.org/). Here, we will show how to obtain meaningful information from the output CSV file and store the information which Front-End might need in the MongoDB. The output CSV files are under `ttk/output_csv_from_ttk/` folder. To obtain meaningful information, we need to run `load_from_csv_mergetree.py` by giving a CSV file path.

```shell
python3 load_from_csv_mergetree.py --file=<input CSV file path>
```

A script file which contains several example commands can be run as following.

```shell
sh run_load_from_csv_mergetree.sh
```

All the results will be saved in the MongoDB. `load_from_csv_mergetree_nersc.py` and `run_load_from_csv_mergetree_nersc.sh` are designed for running this script in `NERSC`. `load_from_csv_mergetree_nersc_training.py` and `run_load_from_csv_mergetree_nersc_training.sh` are designed for running the 3D loss landscapes merge tree in `NERSC`.

### TTK Persistant Diagram Instructions
Another visualization in the 3D loss landscapes is the `Persistant Diagram`, which is also generated by the [TTK](https://topology-tool-kit.github.io/). We will generated the binary files under the `ttk/input_binary_for_ttk/` folder while running the `calculate_mlp_mnist.py`. `TTK` will take the binary files to generate a CSV file in the [ParaView](https://www.paraview.org/). Here, we will show how to obtain meaningful information from the output CSV file and store the information which Front-End might need in the MongoDB. The output CSV files are under `ttk/output_csv_from_ttk/` folder. To obtain meaningful information, we need to run `load_from_csv_persistantdiagram.py` by giving a CSV file path.

```shell
python3 load_from_csv_persistantdiagram.py --file=<input CSV file path>
```

A script file which contains several example commands can be run as following.

```shell
sh run_load_from_csv_persistantdiagram.sh
```

All the results will be saved in the MongoDB. `load_from_csv_persistantdiagram_nersc.py` and `run_load_from_csv_persistantdiagram_nersc.sh` are designed for running this script in `NERSC`. `load_from_csv_persistantdiagram_nersc_training.py` and `run_load_from_csv_persistantdiagram_nersc_training.sh` are designed for running the 3D loss landscapes merge tree in `NERSC`.

### Run Different Functions Separately
We provide several Python scripts, which contains different backend functions. Due to computational resources, sometimes personal computers may not be able to run all the backend functions of one case study in one Python script perfectly. Therefore, if there are issues such as excessive runtime and insufficient memory while running `calculate_torchvision_cifar10.py`, we suggest running different backend functions separately, especially when dealing with large models. Each individual part follows a naming convention similar to `calculate_torchvision_cifar10.py`, followed by the addition of function names for the individual parts for easy identification, such as `calculate_torchvision_cifar10_layer_similarity.py`, `calculate_torchvision_cifar10_loss_landscapes_3d.py` and `calculate_torchvision_cifar10_model_evaluation.py`.

## ViT model with CIFAR10/CIFAR10-C

### Training model Instructions
This example will train several ViT models with [CIFAR10](https://pytorch.org/vision/stable/generated/torchvision.datasets.CIFAR10.html#torchvision.datasets.CIFAR10) and [CIFAR10-C](https://zenodo.org/record/2535967#.Y5oWsezMJPs) datasets. The model structure is obtained from `vit_pytorch`. The models are defined as follows.

```python
from vit_pytorch import ViT
model = ViT(image_size = 32, patch_size = 4, num_classes = 10, dim = 1024, depth = 6, heads = 16, mlp_dim = 2048, dropout = 0.1, emb_dropout = 0.1)
```

We do provide a script file which contains several commands to train some ViT models. You could just run the following command to obtain those trained models with different percents of CIFAR10-C or trained your own models with your settings.

```shell
sh run_train_vit_cifar10.sh
```

You could find more details of functions under `operation` folder. The trained model will be generated under the corresponding folder which should be under `model/CIFAR10/VIT` folder.

### Visualization Instructions
In this tool, the main parts of visualization include model and layer similarity(Model Ensemble View, Model Similarity View and Layer Similarity View), model information(Evaluation View), 2D and 3D loss landscapes(Loss Landscape View) and prediction distribuation(Model Analysis View). For model and layer similarity, this tool provides euclidean distance similarity for models only. We also provide linear and kernel [CKA](https://arxiv.org/abs/1905.00414) similarity for models and CKA similarity for layers as well. For the model information for each model, this system does provide some information related to the model and data, which needs to be visualized. The model information mainly includes `accuracy`, `recall`, `precision`, `f1 score`, `confusion matrix`, and `top-2 Hessian eigenvalues`. For 2D and 3D loss landscapes, we do provide the classic 2D loss landscapes for different models and compare the similarity among them using random projection method. Besides, we provide the merge tree and persistant diagram for 3D loss landscapes. For prediction distribuation, different models in the center of previous loss landscapes will predict the testing data and give different predictions. Model Analysis View presents the differences among all those results. You can find more details in the [Back-End Operations Instructions](operation/README.md#back-end-operations-instructions).

### Main Parts Instructions
To run model and layer similarity, model information, 2D loss landscapes and prediction distribuation for the 2-Layers MLP with MNIST, run the following Python script under the current folder.

```shell
python3 calculate_vit_cifar10.py
```

All the results will be saved in the MongoDB and all the generated model files will be saved under the `model_list` folder.

### TTK Merge Tree Instructions
One visualization in the 3D loss landscapes is the `Merge Tree`, which is generated by the [TTK](https://topology-tool-kit.github.io/). We will generated the binary files under the `ttk/input_binary_for_ttk/` folder while running the `calculate_mlp_mnist.py`. `TTK` will take the binary files to generate a CSV file in the [ParaView](https://www.paraview.org/). Here, we will show how to obtain meaningful information from the output CSV file and store the information which Front-End might need in the MongoDB. The output CSV files are under `ttk/output_csv_from_ttk/` folder. To obtain meaningful information, we need to run `load_from_csv_mergetree.py` by giving a CSV file path.

```shell
python3 load_from_csv_mergetree.py --file=<input CSV file path>
```

A script file which contains several example commands can be run as following.

```shell
sh run_load_from_csv_mergetree.sh
```

All the results will be saved in the MongoDB. `load_from_csv_mergetree_nersc.py` and `run_load_from_csv_mergetree_nersc.sh` are designed for running this script in `NERSC`. `load_from_csv_mergetree_nersc_training.py` and `run_load_from_csv_mergetree_nersc_training.sh` are designed for running the 3D loss landscapes merge tree in `NERSC`.

### TTK Persistant Diagram Instructions
Another visualization in the 3D loss landscapes is the `Persistant Diagram`, which is also generated by the [TTK](https://topology-tool-kit.github.io/). We will generated the binary files under the `ttk/input_binary_for_ttk/` folder while running the `calculate_mlp_mnist.py`. `TTK` will take the binary files to generate a CSV file in the [ParaView](https://www.paraview.org/). Here, we will show how to obtain meaningful information from the output CSV file and store the information which Front-End might need in the MongoDB. The output CSV files are under `ttk/output_csv_from_ttk/` folder. To obtain meaningful information, we need to run `load_from_csv_persistantdiagram.py` by giving a CSV file path.

```shell
python3 load_from_csv_persistantdiagram.py --file=<input CSV file path>
```

A script file which contains several example commands can be run as following.

```shell
sh run_load_from_csv_persistantdiagram.sh
```

All the results will be saved in the MongoDB. `load_from_csv_persistantdiagram_nersc.py` and `run_load_from_csv_persistantdiagram_nersc.sh` are designed for running this script in `NERSC`. `load_from_csv_persistantdiagram_nersc_training.py` and `run_load_from_csv_persistantdiagram_nersc_training.sh` are designed for running the 3D loss landscapes merge tree in `NERSC`.

### Run Different Functions Separately
We provide several Python scripts, which contains different backend functions. Due to computational resources, sometimes personal computers may not be able to run all the backend functions of one case study in one Python script perfectly. Therefore, if there are issues such as excessive runtime and insufficient memory while running `calculate_vit_cifar10.py`, we suggest running different backend functions separately, especially when dealing with large models. Each individual part follows a naming convention similar to `calculate_vit_cifar10.py`, followed by the addition of function names for the individual parts for easy identification, such as `calculate_vit_cifar10_layer_similarity.py`, `calculate_vit_cifar10_loss_landscapes_3d.py` and `calculate_vit_cifar10_model_evaluation.py`.

## Run Python Scripts on NERSC
We provide several shell scripts for running on `NERSC`. If you have a `NERSC` account, you should be able to run the scripts on `NERSC` with more computational resources than your local machine. If the shell script's name contains `nersc`, it is built for running on `NERSC`. Otherwise, it is a script for running in the local environment.